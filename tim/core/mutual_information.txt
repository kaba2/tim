Mutual information
==================

[[Parent]]: entropy_combination.txt

Theory
------

Let ''X'' and ''Y'' be random variables. 
_Mutual information_ is defined by:

''I(X, Y) = H(X) + H(Y) - H(X, Y)''

where ''H'' is the differential entropy. Mutual information measures 
the amount of information shared by ''X'' and ''Y''. It's importance 
lies in the fact that 

''I(X, Y) = 0 <=>'' ''X'' and ''Y'' are independent.

See also
--------

[[Link]]: 
	differential_entropy.txt
	partial_mutual_information.txt
