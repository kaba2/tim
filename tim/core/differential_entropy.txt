Shannon differential entropy
============================

[[Parent]]: entropy.txt

Let ''X'' be a random variable in ''RR^n'' distributed according to 
the probability density function ''mu : RR^n -> RR''. 
The _differential entropy_ of ''X'' is defined by:

''H(X) = -int_{RR^n} mu(x) log(mu(x)) dx''

Entropy vs differential entropy
-------------------------------

There is no generalization of Shannon entropy to continuous variables.
Therefore, while the definition of differential entropy looks similar 
to the definition of discrete entropy, it does not generalize its 
properties. The role of differential entropy is two-fold. First, it is 
a syntactic device for describing other information theoretic 
concepts which are defined as combinations of differential 
entropies. Second, when transforming data to minimize mutual 
information, it is equivalent to minimize differential entropies, 
which can be a bit more efficient. This makes the estimation of
differential entropy by itself useful.

### Differential entropy on embedded manifolds

The definition of differential entropy can be generalized by assuming that
''X'' is distributed on a ''d''-dimensional differentiable manifold ''M'' 
in ''RR^n'', ''0 <= d <= n'', with a probability density function ''mu : M -> RR''.
Then differential entropy is defined by:

''H(X) = -int_{M} mu(x) log(mu(x)) dx''

TIM implements estimators for both of these definitions.
