Transfer entropy
================

[[Parent]]: entropy_combination.txt

Theory
------

Let ''X'', ''Y'', and ''w'' be random variables.
Transfer entropy is defined by:

''T(w, X, Y) = H(w, X) + H(X, Y) - H(X) - H(w, X, Y)''

where ''w'' is the future of ''X''. It measures the amount of directed
information flow from ''X'' to ''Y''.

References
----------

_Measuring Information Transfer_, <br />
Thomas Schreiber, <br />
Physical Review Letters, Volume 85, Number 2, 2000.
