Transfer entropy
================

[[Parent]]: entropy_combination.txt

The _transfer entropy_ between the random variables 
''X'', ''Y'', and ''w'' is defined by

''T(w, X, Y) = H(w, X) + H(X, Y) - H(X) - H(w, X, Y)''

where ''H'' is the Shannon differential entropy and ''w'' is 
the future of ''X''. It measures the amount of directed 
information flow from ''X'' to ''Y''.

References
----------

_Measuring Information Transfer_, <br />
Thomas Schreiber, <br />
Physical Review Letters, Volume 85, Number 2, 2000.

See also
--------

[[Link]]: 
	differential_entropy.txt
	partial_transfer_entropy.txt
