Renyi entropy
=============

[[Parent]]: estimators.txt

The _Renyi entropy_ of a continuous random variable ''X'' is defined by

''H_q(X) = (1 / (1 - q)) log(int_{RR^n} f^q(x) dx),''

where ''f : RR^n -> RR'' is the probability density function of ''X'',
and ''q in RR''. As ''q'' approaches 1, the Renyi entropy approaches
the Shannon differential entropy. This limit can be made part of the
definition to make Renyi entropy continuous on ''q''.

