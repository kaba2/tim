Entropy combinations
====================

[[Parent]]: estimators.txt

An _entropy combination_ of a random variable ''X'' is defined by

''C(X) = sum_{k = 1}^p c_k H(X_{L_k})''

where 

 * ''X = (X_1, ..., X_m) in RR^d'',
 * ''H'' is the [differential entropy][DiffEntropy],
 * ''\forall k in [1, p]: L_k sub [1, m] sub ZZ'',
 * ''X_L = [x_{l_1}, ..., x_{l_{|L|}}]'',
 * ''c in RR^m'', and
 * ''sum_{k = 1}^m c_k |L_k| = 0''.

[DiffEntropy]: [[Ref]]: differential_entropy.txt

Practice
--------

The implementation of the entropy combination estimator requires
additionally that each ''L_k'' is an interval. This allows to save
memory by allowing the memory for the joint signal to be
shared with the marginal signals. It is easily seen that mutual
information, partial mutual information, transfer entropy,
and partial transfer entropy can all be arranged to have this property.

