Entropy combinations
====================

[Parent]: estimators.txt

Theory
------

Let ''X = (X_1, ..., X_m)'' be a random variable.
An entropy combination is defined by

''EC(X_{L_1}, ..., X_{L_p}) = [sum_{i = 1}^p s_i H(X_{L_i})] - H(X)''

where ''\forall i in [1, p]: L_i sub [1, m]'' and ''s_i in {-1, 1}'' such that 

''sum_{i = 1}^m s_i chi_{L_i} = chi_{[1, m]}''

where ''chi_S'' is the characteristic function of a set ''S''. 

The estimator used for computing entropy combinations is a 
generalization of the ideas used by Alexander Kraskov in his 
thesis to develop a mutual information estimator (see references).

Practice
--------

The implementation of the entropy combination estimator requires
additionally that each ''L_i'' is an interval. This allows to save
memory by allowing the memory for the joint signal to be
shared with the marginal signals. It is easily seen that mutual
information, partial mutual information, and transfer entropy can 
all be arranged to have this property.

References
----------

_Synchronization and Interdependence Measures and their
Applications to the Electroencephalogram of Epilepsy
Patients and Clustering of Data_, <br />
Alexander Kraskov, <br />
PhD thesis, NIC Series Vol. 24, February 2004.