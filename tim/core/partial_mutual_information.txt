Partial mutual information
==========================

[[Parent]]: entropy_combination.txt

Theory
------

Let ''X'', ''Y'', and ''Z'' be random variables.
_Partial mutual information_ is defined by:

''I(X, Y ; Z) = H(X, Z) + H(Z, Y) - H(Z) - H(X, Z, Y)''

where ''H'' denotes differential entropy.
It measures the amount of information shared by ''X'' and ''Y''
while discounting the possibility that ''Z'' drives both ''X'' and ''Y''.

References
----------

_Partial Mutual Information for Coupling Analysis of Multivariate Time Series_, <br />
Stefan Frenzel and Bernd Pompe, <br />
Physical Review Letters, 2007.

See also
--------

[[Link]]: 
	differential_entropy.txt
	mutual_information.txt
