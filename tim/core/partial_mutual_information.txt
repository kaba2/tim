Partial mutual information
==========================

[Parent]: entropy_combination.txt

Theory
------

Let ''X'', ''Y'', and ''Z'' be random variables.
Partial mutual information is defined by:

''I(X, Y ; Z) = H(X, Z) + H(Z, Y) - H(Z) - H(X, Z, Y)''

where ''H'' denotes differential entropy.
It measures the amount of information shared by ''X'' and ''Y''
while discounting the possibility that ''Z'' drives both ''X'' and ''Y''.

