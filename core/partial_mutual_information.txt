Partial mutual information estimation
=====================================

[Back to Estimators][Parent]

[Parent]: estimators.htm

Theory
------

Let ''X'', ''Y'', and ''Z'' be random variables.
Partial mutual information is defined by:

''I(X, Y ; Z) = H(X Z) + H(Z Y) - H(Z) - H(X Z Y)''

where ''H'' denotes differential entropy.
It measures the amount of information shared by ''X'' and ''Y''
while discounting the possibility that ''Z'' drives both ''X'' and ''Y''.

Files
-----

### Partial mutual information estimation

[partial_mutual_information.cpp][1]

[partial_mutual_information.h][2]

[partial_mutual_information.hpp][3]

[1]: partial_mutual_information.cpp.htm

[2]: partial_mutual_information.h.htm

[3]: partial_mutual_information.hpp.htm

